{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Normal_33/cdf/ndtr/mul_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "dist = tf.distributions.Normal(loc=0., scale=3.)\n",
    "dist.cdf(1.)\n",
    "# Evaluate the cdf at 1, returning a scalar.\n",
    "#sess = tf.Session()\n",
    "#sess.run(dist.cdf(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import quandl\n",
    "# q_api_key = \"bB4wp5--7XrkpGZ7-gxJ\"\n",
    "# quandl.ApiConfig.api_key = q_api_key\n",
    "# MinPercentileDays = 100 \n",
    "# QuandlAuthToken = \"\"  # not necessary, but can be used if desired\n",
    "# Stocks=['GE', 'AMD', 'F', 'AAPL', 'AIG', 'CHK', 'MU', 'MSFT', 'CSCO', 'T']\n",
    "# df = quandl.get_table('WIKI/PRICES', ticker=Stocks, qopts = { 'columns': ['ticker', 'volume','adj_close'] },\n",
    "#                   date = { 'gte': '2009-01-01', 'lte': '2015-1-1' }, paginate=True) \n",
    "# df.to_csv(\"/Users/andrewplaate/mlp3/10Stocks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:policy_gradient:policy_gradient logger started.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'policy_gradient' from '/Users/andrewplaate/mlp3/gym_trading/envs/policy_gradient.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import policy_gradient\n",
    "reload(policy_gradient)\n",
    "\n",
    "#reload(trading_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "13.677830219268799\n",
      "> /Users/andrewplaate/mlp3/gym_trading/envs/policy_gradient.py(446)train_model()\n",
      "-> xs,rs,ys = [],[],[] # reset game history\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:policy_gradient:year #     0, mean reward:  -4.2099, sim ret:   0.0000, mkt ret:   0.0000, net:   0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.874043941497803\n",
      "> /Users/andrewplaate/mlp3/gym_trading/envs/policy_gradient.py(445)train_model()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) exit()\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5d12d1864a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mgrads_clipped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                             feed_dict={pg.X: epX, pg._tf_epr: epr, pg._tf_y: epy, pg._tf_x: epx})'''\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, load_model=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlp3/gym_trading/envs/policy_gradient.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, env, episodes, load_model, model_dir, log_freq)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNomReward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNomReward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnominal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                 \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# reset game history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mlp3/gym_trading/envs/policy_gradient.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, env, episodes, load_model, model_dir, log_freq)\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNomReward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNomReward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnominal_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                 \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# reset game history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mlp/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import gym_trading\n",
    "\n",
    "env = gym.make('trading-v0')\n",
    "#env.time_cost_bps = 0 # \n",
    "\n",
    "env = env.unwrapped\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import policy_gradient\n",
    "\n",
    "\n",
    "# create the tf session\n",
    "\n",
    "\n",
    "# Input\n",
    "num_actions=10  # same as # of stocks\n",
    "Variables=3\n",
    "obs_dim=num_actions*Variables\n",
    "NumOfHiddLayers=2\n",
    "architecture = \"LSTM\" # right now, valid inputs are LSTM and FFNN\n",
    "LR=\"GD\"\n",
    "actFunc=\"relu\"\n",
    "regulizer=\"l2\"\n",
    "regulizerScale=0.0001\n",
    "#avgfilename=\"/home/s1793158/mlp3/FILE_Name.p\"\n",
    "\n",
    "actFuncs=[\"lrelu\"]\n",
    "name=[\"RNN_GD_relu_l2\",\"RNN_GD_elu_l2\",\"RNN_GD_lrelu_l2\",\"RNN_GD_selu_l2\",\"RNN_GD_selu_l2\",\"RNN_GD_sig_l2\"]\n",
    "name=[\"RNN_GD_sig_l2\"]\n",
    "   # from tensorflow.python.framework import ops\n",
    "    #ops.reset.default_graph()\n",
    "\n",
    "for i in range(1):\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.InteractiveSession()\n",
    "    #tf.reset_default_graph()\n",
    " #   tf.set_random_seed(0)\n",
    "  #  with tf.Session() as sess:\n",
    "        tf.set_random_seed(0)\n",
    "    #avgfilename=\"/afs/inf.ed.ac.uk/user/s17/s1793158/mlp3/Saving/\"+name[i]+\".p\"\n",
    "    #Modelfilename=\"/afs/inf.ed.ac.uk/user/s17/s1793158/mlp3/SavedModels/\"+name[i]\n",
    "\n",
    "        avgfilename=\"/Users/andrewplaate/mlp3/Saving/\"+name[i]+\".p\"\n",
    "        Modelfilename=\"/Users/andrewplaate/mlp3/\"+name[i]\n",
    "        sess = tf.InteractiveSession()\n",
    "\n",
    "\n",
    "        pg = policy_gradient.PolicyGradient(sess, obs_dim=obs_dim, \n",
    "                                            num_actions=num_actions,\n",
    "                                            NumOfLayers=NumOfHiddLayers, \n",
    "                                            Num_Of_variables=Variables,\n",
    "                                            LR=LR,\n",
    "                                            architecture=architecture,\n",
    "                                            actFunc=actFuncs[i],\n",
    "                                            learning_rate=1e-4,\n",
    "                                            regulizer =regulizer,\n",
    "                                            regulizerScale=regulizerScale,\n",
    "                                            avgfilename=avgfilename,\n",
    "                                            Modelfilename=Modelfilename,\n",
    "                                            num_hiddenRNN=24,\n",
    "                                            DropoutMemoryStates= True,\n",
    "                                            DropoutVariational_recurrent=True,\n",
    "                                            output_keep_prob=0.8,\n",
    "                                            state_keep_prob=0.8\n",
    "                                           )\n",
    "\n",
    "        # and now let's train it and evaluate its progress.  NB: this could take some time...\n",
    "        direc=\"aa\"\n",
    "        load_model=False\n",
    "        #grads, grads_clipped = pg.get_grads_and_clipping()\n",
    "        '''with tf.Session() as sess:\n",
    "        sess.run(init)  # actually running the initialization op\n",
    "        _grads_clipped, _grads = sess.run(\n",
    "                            [grads_clipped, grads],\n",
    "                            feed_dict={pg.X: epX, pg._tf_epr: epr, pg._tf_y: epy, pg._tf_x: epx})'''\n",
    "        df,sf = pg.train_model(env, episodes=2, log_freq=1, load_model=False,model_dir = direc)#, load_model=True)\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import interactive\n",
    "# interactive(True)\n",
    "\n",
    "# import pickle as pkl\n",
    "# loca=\"/Users/andrewplaate/mlp3/TestRNN/\"\n",
    "\n",
    "# # adam3 = pkl.load( open( loca+\"FFN_Adam3_lrelu_l2.p\", \"rb\" ) )\n",
    "# # adam4 = pkl.load( open( loca+\"FFN_Adam4_lrelu_l2.p\", \"rb\" ) )\n",
    "# # rms3 = pkl.load( open( loca+\"FFN_RMS3_lrelu_l2.p\", \"rb\" ) )\n",
    "# # rms4 = pkl.load( open( loca+\"FFN_RMS4_lrelu_l2.p\", \"rb\" ) )\n",
    "# # gd3 = pkl.load( open( loca+\"FFN_GD3_lrelu_l2.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# # adam3 = pkl.load( open( loca+\"RNN_Adam2_lrelu_l2.p\", \"rb\" ) )\n",
    "# # adam4 = pkl.load( open( loca+\"RNN_Adam3_lrelu_l2.p\", \"rb\" ) )\n",
    "# # rms3 = pkl.load( open( loca+\"RNN_RMS2_lrelu_l2.p\", \"rb\" ) )\n",
    "# # rms4 = pkl.load( open( loca+\"RNN_RMS3_lrelu_l2.p\", \"rb\" ) )\n",
    "# # gd3 = pkl.load( open( loca+\"RNN_GD2_lrelu_l2.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# adam3 = pkl.load( open( loca+\"RNN_Adam_lrelu_l2_e3.p\", \"rb\" ) )\n",
    "# adam4 = pkl.load( open( loca+\"RNN_Adam_lrelu_l2_e4.p\", \"rb\" ) )\n",
    "# rms3 = pkl.load( open( loca+\"RNN_RMSProp_lrelu_l2_e3.p\", \"rb\" ) )\n",
    "# rms4 = pkl.load( open( loca+\"RNN_RMSProp_lrelu_l2_e4.p\", \"rb\" ) )\n",
    "# gd3 = pkl.load( open( loca+\"RNN_GD_lrelu_l2_e3.p\", \"rb\" ) )\n",
    "# gd4 = pkl.load( open( loca+\"RNN_GD_lrelu_l2_e4.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(adam3[0,:],label=\"adam3\")\n",
    "# plt.plot(adam4[0,:],label=\"adam4\")\n",
    "# plt.plot(rms3[0,:],label=\"rms3\")\n",
    "# plt.plot(rms4[0,:],label=\"rms4\")\n",
    "# plt.plot(gd3[0,:],label=\"gd3\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# meanss=pd.DataFrame(np.array([np.mean(adam3,axis=1),np.mean(adam4,axis=1),np.mean(rms3,axis=1),np.mean(rms4,axis=1),\n",
    "#                 np.mean(gd3,axis=1),np.mean(gd4,axis=1)]))\n",
    "\n",
    "# meanss.index= [name1,name2,name3,name4,name5,name6]\n",
    "# meanss.columns=[\"Mean Sort\",\"Mean Ret\"]\n",
    "# meanss\n",
    "\n",
    "\n",
    "# std=pd.DataFrame(np.array([np.std(adam3,axis=1),np.std(adam4,axis=1),np.std(rms3,axis=1),np.std(rms4,axis=1),\n",
    "#                 np.std(gd3,axis=1),np.std(gd4,axis=1)]))\n",
    "\n",
    "# std.index= [name1,name2,name3,name4,name5,name6]\n",
    "# std.columns=[\"Std Sort\",\"Std Ret\"]\n",
    "# std\n",
    "# DF=meanss.join(std)\n",
    "\n",
    "# DF=DF.loc[:,[\"Mean Sort\",\"Std Sort\",\"Mean Ret\",\"Std Ret\"]]\n",
    "\n",
    "# print(DF.round(4).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# name1=\"Adam(1e-3)\"\n",
    "# name2=\"Adam(1e-4)\"\n",
    "# name3=\"RMSP(1e-3)\"\n",
    "# name4=\"RMSP(1e-4)\"\n",
    "# name5=\"SGD(1e-3)\"\n",
    "# name6=\"SGD(1e-4)\"\n",
    "\n",
    "\n",
    "# aa=np.arange(-0.05,0.05,0.001)\n",
    "# aa=100\n",
    "# alphaa=0.3\n",
    "\n",
    "# SortRet=0\n",
    "\n",
    "\n",
    "# #plt.title(\"Total Returns\")\n",
    "# plt.figure(figsize=(12,6))\n",
    "\n",
    "# plt.title(\"Sortino Ratio\", size=18)\n",
    "# #plt.title(\"Total Returns\",size=18)\n",
    "# plt.hist(adam3[SortRet,:],label= name1,alpha = alphaa,bins=aa)\n",
    "# plt.hist(adam4[SortRet,:],label=name2,alpha = alphaa,bins=aa)\n",
    "# plt.hist(rms3[SortRet,:],label=name3, alpha = alphaa,bins=aa)\n",
    "# plt.hist(rms4[SortRet,:],label=name4, alpha =alphaa,bins=aa)\n",
    "# plt.hist(gd3[SortRet,:],label=name5, alpha = alphaa,bins=aa)\n",
    "# plt.hist(gd4[SortRet,:],label=name6, alpha = alphaa,bins=aa)\n",
    "# plt.xticks(size=20)\n",
    "# plt.yticks(size=20)\n",
    "# plt.ylabel(\"Count\",size=20)\n",
    "# plt.legend( fontsize=20)\n",
    "# plt.savefig('/Users/andrewplaate/mlp3/gym_trading/envs/'+\"RNN_Sort_LR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
