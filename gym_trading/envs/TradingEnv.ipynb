{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TradingEnv-v0\n",
    "\n",
    "### Open AI 'Gym' for reinforcement-learning based trading algorithms\n",
    "\n",
    "This gym implements a very simple trading environment for reinforcement learning.\n",
    "\n",
    "The gym provides daily observations based on real market data pulled from Quandl on, by default, the SPY etf.  An episode is defined as 252 contiguous days sampled from the overall dataset.  Each day is one 'step' within the gym and for each step, the algo has a choice:\n",
    "\n",
    " - SHORT (0)\n",
    " - FLAT (1)\n",
    " - LONG  (2)\n",
    " \n",
    "If you trade, you will be charged, by default, 10 BPS of the size of your trade.  Thus, going from short to long costs twice as much as going from short to/from flat.  Not trading also has a default cost of 1 BPS per step.  Nobody said it would be easy!\n",
    " \n",
    "At the beginning of your episode, you are allocated 1 unit of cash.  This is your starting Net Asset Value (NAV). \n",
    "\n",
    "### Beating the trading game \n",
    "\n",
    "For our purposes, we'll say that beating a buy & hold strategy, on average, over one hundred episodes will notch a win to the proud ai player.  We'll illustrate exactly what that means below.\n",
    "\n",
    "### Let's look at some code using the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import interactive\n",
    "interactive(True)\n",
    "import gym_trading\n",
    "\n",
    "env = gym.make('trading-v0')\n",
    "#env.time_cost_bps = 0 # \n",
    "\n",
    "env = env.unwrapped\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import policy_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the tf session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Input\n",
    "num_actions=3  # same as # of stocks\n",
    "Variables=3\n",
    "obs_dim=num_actions*Variables\n",
    "NumOfLayers=3\n",
    "LR=\"Adam\"\n",
    "actFunc=\"relu\"\n",
    "regulizer=\"l2\"\n",
    "regulizerScale=0.01\n",
    "\n",
    "\n",
    "pg = policy_gradient.PolicyGradient(sess, obs_dim=obs_dim, \n",
    "                                    num_actions=num_actions,\n",
    "                                    NumOfLayers=NumOfLayers, \n",
    "                                    LR=LR,\n",
    "                                    actFunc=actFunc,\n",
    "                                    learning_rate=1e-2,\n",
    "                                    regulizer =regulizer,\n",
    "                                    regulizerScale=regulizerScale\n",
    "                                   )\n",
    "\n",
    "# and now let's train it and evaluate its progress.  NB: this could take some time...\n",
    "direc=\"/Users/andrewplaate/mlp3/SavedModels/\"\n",
    "load_model=True\n",
    "df,sf = pg.train_model(env, episodes=25001, log_freq=100, load_model=load_model,model_dir = direc)#, load_model=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Policy gradients beat the trading game!  That said, it doesn't work every time and it seems, looking at the charts below, as though it's a bit of a lucky thing.  But luck counts in the trading game as in life!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf['net'] = sf.simror - sf.mktror\n",
    "#sf.net.plot()\n",
    "sf.net.expanding().mean().plot()\n",
    "sf.net.rolling(100).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf.net.rolling(100).mean().tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
